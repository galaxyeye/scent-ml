{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:07.796531400Z",
     "start_time": "2023-11-24T15:42:07.792021300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataLoadTest\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:07.865767900Z",
     "start_time": "2023-11-24T15:42:07.795531300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\Users\\\\pereg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe'"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:07.869381400Z",
     "start_time": "2023-11-24T15:42:07.866772400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "path = \"../../data/dom/amazon.dataset.libsvm.11.24.50.txt\"\n",
    "lines = sc.textFile(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:07.909516600Z",
     "start_time": "2023-11-24T15:42:07.870422400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "c = lines.count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:08.924533100Z",
     "start_time": "2023-11-24T15:42:07.890002500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "rdd = lines\\\n",
    "    .filter(lambda line: line[0] != \"#\")\\\n",
    "    .map(lambda line: (line.split(\" ^|^ \"))).filter(lambda record: len(record) == 4)\\\n",
    "    .filter(lambda record: len(record) == 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:08.928894800Z",
     "start_time": "2023-11-24T15:42:08.925532700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "[['-1',\n  ' 1:999.7 2:1501.1 3:42.8 4:15.3 5:6 11:31 12:3474 33:6 34:6 35:6 64:42.8 65:15.3 70:999.7 71:1501.1 72:42.8 73:15.3 74:6 75:1 78:2 80:30 81:3473 82:253.9683 90:999.7 91:1501.1 95:1 102:6 103:6 104:6 105:1 109:3473 122:42.8 124:15.3 133:42.8 134:15.3 139:997.7 140:1364.3 141:179.6 142:20 143:5 149:30 150:3472 171:5 172:5 173:5 202:179.6 203:20 208:997.7 209:1364.3 210:179.6 211:20 212:5 218:30 219:3472 240:5 241:5 242:5 271:179.6 272:20',\n  '$17.99',\n  'https://www.amazon.com/dp/B0148NNKTC'],\n ['-1',\n  ' 1:999.7 2:1369.3 3:107.7 4:15.3 5:17 11:31 12:3471 33:17 34:17 35:17 64:107.7 65:15.3 70:999.7 71:1369.3 72:107.7 73:15.3 74:17 75:1 77:1 78:2 80:30 81:3470 82:99.6885 90:999.7 91:1369.3 95:1 102:17 103:17 104:17 105:1 109:3470 122:107.7 124:15.3 126:107.7 128:15.3 133:107.7 134:15.3 135:107.7 136:15.3 139:997.7 140:1364.3 141:179.6 142:20 149:30 150:3469 208:997.7 209:1364.3 210:179.6 211:20 218:30 219:3469',\n  '4-Year Protection',\n  'https://www.amazon.com/dp/B0148NNKTC'],\n ['-1',\n  ' 1:999.5 2:636.7 3:667 4:28 5:17 11:11 12:6403 33:17 34:17 35:17 64:667 65:28 70:999.5 71:636.7 72:667 73:28 74:17 75:1 78:3 80:10 81:6402 82:8.5671 90:999.5 91:636.7 95:1 102:17 103:17 104:17 105:1 109:6402 122:667 124:28 133:667 134:28 139:984.5 140:636.7 141:667 142:303 149:10 150:6401 208:984.5 209:636.7 210:667 211:303 218:10 219:6401',\n  'About this item',\n  'https://www.amazon.com/dp/B014I8SIJY'],\n ['-1',\n  ' 1:999.4 2:1506 3:44.5 4:17.3 5:6 11:30 12:3291 33:6 34:6 35:6 64:44.5 65:17.3 70:999.4 71:1506 72:44.5 73:17.3 74:6 75:1 78:2 80:29 81:3290 82:213.9037 90:999.4 91:1506 95:1 102:6 103:6 104:6 105:1 109:3290 122:44.5 124:17.3 133:44.5 134:17.3 139:998.1 140:1364.3 141:186.2 142:20 143:5 149:29 150:3289 171:5 172:5 173:5 202:186.2 203:20 208:998.1 209:1364.3 210:186.2 211:20 212:5 218:29 219:3289 240:5 241:5 242:5 271:186.2 272:20',\n  '$32.99',\n  'https://www.amazon.com/dp/B01CX26WIG'],\n ['-1',\n  ' 1:999.4 2:1369.3 3:110.8 4:17.3 5:17 11:30 12:3288 33:17 34:17 35:17 64:110.8 65:17.3 70:999.4 71:1369.3 72:110.8 73:17.3 74:17 75:1 77:1 78:2 80:29 81:3287 82:85.5615 90:999.4 91:1369.3 95:1 102:17 103:17 104:17 105:1 109:3287 122:110.8 124:17.3 126:110.8 128:17.3 133:110.8 134:17.3 135:110.8 136:17.3 139:998.1 140:1364.3 141:186.2 142:20 149:29 150:3286 208:998.1 209:1364.3 210:186.2 211:20 218:29 219:3286',\n  '4-Year Protection',\n  'https://www.amazon.com/dp/B01CX26WIG']]"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:09.855500100Z",
     "start_time": "2023-11-24T15:42:08.929893900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:09.857721600Z",
     "start_time": "2023-11-24T15:42:09.854996100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda r: (r[0], r[1], r[2], r[3]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:09.859245Z",
     "start_time": "2023-11-24T15:42:09.857511100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|    numeric_features|                text|                 url|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   -1| 1:231 2:1348.3 3...|Prime Exclusive Deal|https://www.amazo...|\n",
      "|   -1| 1:261.3 2:1348.3...|              $21.59|https://www.amazo...|\n",
      "|   -1| 1:258.6 2:1348.3...|                   $|https://www.amazo...|\n",
      "|   -1| 1:253.7 2:1356 3...|                  21|https://www.amazo...|\n",
      "|   -1| 1:253.7 2:1388.8...|                   .|https://www.amazo...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2.toDF([\"label\", \"numeric_features\", \"text\", \"url\"]).show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:10.856138700Z",
     "start_time": "2023-11-24T15:42:09.860241100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from ml.dom.data.DataUtils import DataUtils\n",
    "\n",
    "rdd3 = rdd.map(lambda r: (r[0], DataUtils.parse_libsvm_line_to_point(r[1]), r[2], r[3]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:10.859705600Z",
     "start_time": "2023-11-24T15:42:10.856138700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:10.861602Z",
     "start_time": "2023-11-24T15:42:10.860596300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `numeric_features`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1630\u001B[0m, in \u001B[0;36m_infer_type\u001B[1;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1629\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1634\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1670\u001B[0m, in \u001B[0;36m_infer_schema\u001B[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1669\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1670\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   1671\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1672\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(row)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m   1673\u001B[0m     )\n\u001B[0;32m   1675\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1681\u001B[0m, in \u001B[0;36m_infer_schema\u001B[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1678\u001B[0m     fields\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m   1679\u001B[0m         StructField(\n\u001B[0;32m   1680\u001B[0m             k,\n\u001B[1;32m-> 1681\u001B[0m             \u001B[43m_infer_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1682\u001B[0m \u001B[43m                \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1683\u001B[0m \u001B[43m                \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1684\u001B[0m \u001B[43m                \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1685\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1686\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m   1687\u001B[0m             \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   1688\u001B[0m         )\n\u001B[0;32m   1689\u001B[0m     )\n\u001B[0;32m   1690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1636\u001B[0m, in \u001B[0;36m_infer_type\u001B[1;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m-> 1636\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   1637\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNSUPPORTED_DATA_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1638\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m   1639\u001B[0m     )\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1630\u001B[0m, in \u001B[0;36m_infer_type\u001B[1;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1629\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1631\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1632\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1634\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1691\u001B[0m, in \u001B[0;36m_infer_schema\u001B[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1690\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m-> 1691\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   1692\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1693\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: k},\n\u001B[0;32m   1694\u001B[0m         )\n\u001B[0;32m   1695\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m StructType(fields)\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `_1`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1681\u001B[0m, in \u001B[0;36m_infer_schema\u001B[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1678\u001B[0m     fields\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m   1679\u001B[0m         StructField(\n\u001B[0;32m   1680\u001B[0m             k,\n\u001B[1;32m-> 1681\u001B[0m             \u001B[43m_infer_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1682\u001B[0m \u001B[43m                \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1683\u001B[0m \u001B[43m                \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1684\u001B[0m \u001B[43m                \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1685\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1686\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m   1687\u001B[0m             \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   1688\u001B[0m         )\n\u001B[0;32m   1689\u001B[0m     )\n\u001B[0;32m   1690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1636\u001B[0m, in \u001B[0;36m_infer_type\u001B[1;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m-> 1636\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   1637\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUNSUPPORTED_DATA_TYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1638\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m   1639\u001B[0m     )\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `tuple`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[126], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df3 \u001B[38;5;241m=\u001B[39m \u001B[43mrdd3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoDF\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnumeric_features\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43murl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df3\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:122\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[1;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     89\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1443\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[0;32m   1442\u001B[0m     )\n\u001B[1;32m-> 1443\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1445\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1483\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[1;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[0;32m   1480\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[0;32m   1482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[1;32m-> 1483\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1485\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1056\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[1;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1053\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[0;32m   1054\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m-> 1056\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1057\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[0;32m   1058\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1007\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[1;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[0;32m   1005\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1007\u001B[0m     schema \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1013\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[0;32m   1014\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m100\u001B[39m)[\u001B[38;5;241m1\u001B[39m:]:\n",
      "File \u001B[1;32mD:\\Users\\pereg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1691\u001B[0m, in \u001B[0;36m_infer_schema\u001B[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[0;32m   1678\u001B[0m         fields\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m   1679\u001B[0m             StructField(\n\u001B[0;32m   1680\u001B[0m                 k,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1688\u001B[0m             )\n\u001B[0;32m   1689\u001B[0m         )\n\u001B[0;32m   1690\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m-> 1691\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m   1692\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1693\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: k},\n\u001B[0;32m   1694\u001B[0m         )\n\u001B[0;32m   1695\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m StructType(fields)\n",
      "\u001B[1;31mPySparkTypeError\u001B[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `numeric_features`."
     ]
    }
   ],
   "source": [
    "df3 = rdd3.toDF([\"label\", \"numeric_features\", \"text\", \"url\"])\n",
    "df3.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T15:42:11.431273400Z",
     "start_time": "2023-11-24T15:42:10.862601800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
